
extern unsigned int kobjsize(const void *objp);
#endif

/*
 * vm_flags in vm_area_struct, see mm_types.h.
 */
#define VM_READ 0x00000001	/* currently active flags */
#define VM_WRITE 0x00000002
#define VM_EXEC 0x00000004
#define VM_SHARED 0x00000008

/* mprotect() hardcodes VM_MAYREAD >> 4 == VM_READ, and so for r/w/x bits. */
#define VM_MAYREAD 0x00000010	/* limits for mprotect() etc */
#define VM_MAYWRITE 0x00000020
#define VM_MAYEXEC 0x00000040
#define VM_MAYSHARE 0x00000080

#define VM_GROWSDOWN 0x00000100	/* general info on the segment */
#if defined(CONFIG_STACK_GROWSUP) || defined(CONFIG_IA64)
#define VM_GROWSUP 0x00000200
#else
#define VM_GROWSUP 0x00000000
#define VM_NOHUGEPAGE 0x00000200	/* MADV_NOHUGEPAGE marked this vma */
#endif
#define VM_PFNMAP 0x00000400	/* Page-ranges managed without "struct page", just pure PFN */
#define VM_DENYWRITE 0x00000800	/* ETXTBSY on write attempts.. */

#define VM_EXECUTABLE 0x00001000
#define VM_LOCKED 0x00002000
#define VM_IO 0x00004000	/* Memory mapped I/O or similar */

					/* Used by sys_madvise() */
#define VM_SEQ_READ 0x00008000	/* App will access data sequentially */
#define VM_RAND_READ 0x00010000	/* App will not benefit from clustered reads */

#define VM_DONTCOPY 0x00020000      /* Do not copy this vma on fork */
#define VM_DONTEXPAND 0x00040000	/* Cannot expand with mremap() */
#define VM_RESERVED 0x00080000	/* Count as reserved_vm like IO */
#define VM_ACCOUNT 0x00100000	/* Is a VM accounted object */
#define VM_NORESERVE 0x00200000	/* should the VM suppress accounting */
#define VM_HUGETLB 0x00400000	/* Huge TLB Page VM */
#define VM_NONLINEAR 0x00800000	/* Is non-linear (remap_file_pages) */
#ifndef CONFIG_TRANSPARENT_HUGEPAGE
#define VM_MAPPED_COPY 0x01000000	/* T if mapped copy of data (nommu mmap) */
#else
#define VM_HUGEPAGE 0x01000000	/* MADV_HUGEPAGE marked this vma */
#endif
#define VM_INSERTPAGE 0x02000000	/* The vma has had "vm_insert_page()" done on it */
#define VM_ALWAYSDUMP 0x04000000	/* Always include in core dumps */

#define VM_CAN_NONLINEAR 0x08000000	/* Has ->fault & does nonlinear pages */
#define VM_MIXEDMAP 0x10000000	/* Can contain "struct page" and pure PFN pages */
#define VM_SAO 0x20000000	/* Strong Access Ordering (powerpc) */
#define VM_PFN_AT_MMAP 0x40000000	/* PFNMAP vma that is fully mapped at mmap time */
#define VM_MERGEABLE 0x80000000	/* KSM may merge identical pages */

/* Bits set in the VMA until the stack is in its final location */
#define VM_STACK_INCOMPLETE_SETUP (VM_RAND_READ | VM_SEQ_READ)

#ifndef VM_STACK_DEFAULT_FLAGS		/* arch can override this */
#define VM_STACK_DEFAULT_FLAGS VM_DATA_DEFAULT_FLAGS
#endif

#ifdef CONFIG_STACK_GROWSUP
#define VM_STACK_FLAGS (VM_GROWSUP | VM_STACK_DEFAULT_FLAGS | VM_ACCOUNT)
#else
#define VM_STACK_FLAGS (VM_GROWSDOWN | VM_STACK_DEFAULT_FLAGS | VM_ACCOUNT)
#endif

#define VM_READHINTMASK (VM_SEQ_READ | VM_RAND_READ)
#define VM_ClearReadHint(v) (v)->vm_flags &= ~VM_READHINTMASK
#define VM_NormalReadHint(v) (!((v)->vm_flags & VM_READHINTMASK))
#define VM_SequentialReadHint(v) ((v)->vm_flags & VM_SEQ_READ)
#define VM_RandomReadHint(v) ((v)->vm_flags & VM_RAND_READ)

/*
 * special vmas that are non-mergable, non-mlock()able
 */
#define VM_SPECIAL (VM_IO | VM_DONTEXPAND | VM_RESERVED | VM_PFNMAP)

/*
 * mapping from the currently active vm_flags protection bits (the
 * low four bits) to a page protection mask..
 */
extern pgprot_t protection_map[16];

#define FAULT_FLAG_WRITE 0x01	/* Fault was a write access */
#define FAULT_FLAG_NONLINEAR 0x02	/* Fault was via a nonlinear mapping */
#define FAULT_FLAG_MKWRITE 0x04	/* Fault was mkwrite of existing pte */
#define FAULT_FLAG_ALLOW_RETRY 0x08	/* Retry fault if blocking */
#define FAULT_FLAG_RETRY_NOWAIT 0x10	/* Don't drop mmap_sem and wait when retrying */

/*
 * This interface is used by x86 PAT code to identify a pfn mapping that is
 * linear over entire vma. This is to optimize PAT code that deals with
 * marking the physical region with a particular prot. This is not for generic
 * mm use. Note also that this check will not work if the pfn mapping is
 * linear for a vma starting at physical address 0. In which case PAT code
 * falls back to slow path of reserving physical range page by page.
 */
static inline int is_linear_pfn_mapping(struct vm_area_struct *vma)
{
	return (vma->vm_flags & VM_PFN_AT_MMAP);
}