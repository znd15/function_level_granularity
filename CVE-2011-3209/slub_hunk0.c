



/*
 * SLUB: A slab allocator that limits cache line use instead of queuing
 * objects in per cpu and per node lists.
 *
 * The allocator synchronizes using per slab locks and only
 * uses a centralized lock to manage a pool of partial slabs.
 *
 * (C) 2007 SGI, Christoph Lameter <clameter@sgi.com>
 */

#include <linux/mm.h>
#include <linux/module.h>
#include <linux/bit_spinlock.h>
#include <linux/interrupt.h>
#include <linux/bitops.h>
#include <linux/slab.h>
#include <linux/seq_file.h>
#include <linux/cpu.h>
#include <linux/cpuset.h>
#include <linux/mempolicy.h>
#include <linux/ctype.h>
#include <linux/debugobjects.h>
#include <linux/kallsyms.h>
#include <linux/memory.h>
#include <linux/math64.h>

/*
 * Lock order:
 * 1. slab_lock(page)
 * 2. slab->list_lock
 *
 * The slab_lock protects operations on the object of a particular
 * slab and its metadata in the page struct. If the slab lock
 * has been taken then no allocations nor frees can be performed
 * on the objects in the slab nor can the slab be added or removed
 * from the partial or full lists since this would mean modifying
 * the page_struct of the slab.
 *
 * The list_lock protects the partial and full list on each node and
 * the partial slab counter. If taken then no new slabs may be added or
 * removed from the lists nor make the number of partial slabs be modified.
 * (Note that the total number of slabs is an atomic value that may be
 * modified without taking the list lock).
 *
 * The list_lock is a centralized lock and thus we avoid taking it as
 * much as possible. As long as SLUB does not have to handle partial
 * slabs, operations can continue without any centralized lock. F.e.
 * allocating a long series of objects that fill up slabs does not require
 * the list lock.
 *
 * The lock order is sometimes inverted when we are trying to get a slab
 * off a list. We take the list_lock and then look for a page on the list
 * to use. While we do that objects in the slabs may be freed. We can
 * only operate on the slab if we have also taken the slab_lock. So we use
 * a slab_trylock() on the slab. If trylock was successful then no frees
 * can occur anymore and we can use the slab for allocations etc. If the
 * slab_trylock() does not succeed then frees are in progress in the slab and
 * we must stay away from it for a while since we may cause a bouncing
 * cacheline if we try to acquire the lock. So go onto the next slab.
 * If all pages are busy then we may allocate a new slab instead of reusing
 * a partial slab. A new slab has noone operating on it and thus there is
 * no danger of cacheline contention.
 *
 * Interrupts are disabled during allocation and deallocation in order to
 * make the slab allocator safe to use in the context of an irq. In addition
 * interrupts are disabled to ensure that the processor does not change
 * while handling per_cpu slabs, due to kernel preemption.
 *
 * SLUB assigns one slab for allocation to each processor.
 * Allocations only occur from these slabs called cpu slabs.
 *
 * Slabs with free elements are kept on a partial list and during regular
 * operations no list for full slabs is used. If an object in a full slab is
 * freed then the slab will show up again on the partial lists.
 * We track full slabs for debugging purposes though because otherwise we
 * cannot scan all objects.
 *
 * Slabs are freed when they become empty. Teardown and setup is
 * minimal so we rely on the page allocators per cpu caches for
 * fast frees and allocs.
 *
 * Overloading of page flags that are otherwise used for LRU management.
 *
 * PageActive The slab is frozen and exempt from list processing.
 * This means that the slab is dedicated to a purpose
 * such as satisfying allocations for a specific
 * processor. Objects may be freed in the slab while
 * it is frozen but slab_free will then skip the usual
 * list operations. It is up to the processor holding
 * the slab to integrate the slab into the slab lists
 * when the slab is no longer needed.
 *
 * One use of this flag is to mark slabs that are
 * used for allocations. Then such a slab becomes a cpu
 * slab. The cpu slab may be equipped with an additional
 * freelist that allows lockless access to
 * free objects in addition to the regular freelist
 * that requires the slab lock.
 *
 * PageError Slab requires special handling due to debug
 * options set. This moves slab handling out of
 * the fast path and disables lockless freelists.
 */

#define FROZEN (1 << PG_active)

#ifdef CONFIG_SLUB_DEBUG
#define SLABDEBUG (1 << PG_error)
#else
#define SLABDEBUG 0
#endif

static inline int SlabFrozen(struct page *page)
{
	return page->flags & FROZEN;
}