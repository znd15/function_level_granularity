
bool tcp_add_backlog(struct sock *sk, struct sk_buff *skb)
{
	u32 limit = sk->sk_rcvbuf + sk->sk_sndbuf;

	/* Only socket owner can try to collapse/prune rx queues
 * to reduce memory overhead, so add a little headroom here.
 * Few sockets backlog are possibly concurrently non empty.
 */
	limit += 64*1024;

	/* In case all data was pulled from skb frags (in __pskb_pull_tail()),
 * we can fix skb->truesize to its real value to avoid future drops.
 * This is valid because skb is not yet charged to the socket.
 * It has been noticed pure SACK packets were sometimes dropped
 * (if cooked by drivers without copybreak feature).
 */
	if (!skb->data_len)
		skb->truesize = SKB_TRUESIZE(skb_end_offset(skb));

	if (unlikely(sk_add_backlog(sk, skb, limit))) {
		bh_unlock_sock(sk);
		__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPBACKLOGDROP);
		return true;
	}
	return false;
}